#!/usr/bin/env python3
"""
Test Gemini API Connection
"""

import os
from dotenv import load_dotenv
import google.generativeai as genai

# Load environment variables
load_dotenv()

GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')
LLM_MODEL = os.getenv('LLM_MODEL', 'gemini-2.0-flash-exp')

def test_gemini_connection():
    """Test Gemini API connection with a simple conversation"""

    print("\n" + "="*60)
    print("  ğŸ¤– GEMINI API CONNECTION TEST")
    print("="*60)

    # Check API key
    if not GOOGLE_API_KEY:
        print("\nâŒ Error: GOOGLE_API_KEY not found in .env file")
        return False

    print(f"\nğŸ”‘ API Key: {GOOGLE_API_KEY[:20]}...")
    print(f"ğŸ“¦ Model: {LLM_MODEL}")

    try:
        # Configure API
        genai.configure(api_key=GOOGLE_API_KEY)
        print("\nâœ… API configured successfully")

        # List available models
        print("\nğŸ“‹ Available Models:")
        try:
            models = genai.list_models()
            count = 0
            for model in models:
                if 'generateContent' in model.supported_generation_methods:
                    print(f"   - {model.name}")
                    count += 1
                    if count >= 5:  # Show only first 5 models
                        print("   ... (more models available)")
                        break
        except Exception as e:
            print(f"   âš ï¸  Could not list models: {e}")

        # Test simple conversation
        print("\n" + "="*60)
        print("  ğŸ’¬ TEST CONVERSATION")
        print("="*60)

        print(f"\nğŸ§ª Testing model: {LLM_MODEL}")
        print("ğŸ“ Prompt: 'Hello! Please respond in Korean. What is 1+1?'")

        # Create model instance
        model = genai.GenerativeModel(LLM_MODEL)

        # Generate response
        print("\nâ³ Generating response...")
        response = model.generate_content("Hello! Please respond in Korean. What is 1+1?")

        print("\nâœ… Response received!")
        print("\n" + "-"*60)
        print("ğŸ¤– Gemini Response:")
        print("-"*60)
        print(response.text)
        print("-"*60)

        # Test with system instruction (for KSUR context)
        print("\n" + "="*60)
        print("  ğŸ’¬ TEST WITH SYSTEM INSTRUCTION")
        print("="*60)

        print("\nğŸ§ª Testing with KSUR medical context...")
        print("ğŸ“ Prompt: 'What is PSUR in pharmacovigilance?'")

        model_with_instruction = genai.GenerativeModel(
            LLM_MODEL,
            system_instruction="You are a pharmaceutical regulatory expert specializing in PSUR (Periodic Safety Update Report) analysis."
        )

        print("\nâ³ Generating response...")
        response2 = model_with_instruction.generate_content(
            "What is PSUR in pharmacovigilance? Please answer briefly in Korean."
        )

        print("\nâœ… Response received!")
        print("\n" + "-"*60)
        print("ğŸ¤– Gemini Response (with system instruction):")
        print("-"*60)
        print(response2.text)
        print("-"*60)

        # Token count test
        print("\n" + "="*60)
        print("  ğŸ“Š TOKEN COUNT TEST")
        print("="*60)

        test_text = "This is a test message for token counting in KSUR system."
        print(f"\nğŸ“ Text: '{test_text}'")

        try:
            token_count = model.count_tokens(test_text)
            print(f"\nâœ… Token count: {token_count.total_tokens}")
        except Exception as e:
            print(f"\nâš ï¸  Token counting not available: {e}")

        print("\n" + "="*60)
        print("  âœ… ALL TESTS PASSED!")
        print("="*60)
        print("\nâœ… Gemini API is working correctly")
        print("âœ… Ready for KSUR system integration")
        print("\n")

        return True

    except Exception as e:
        print(f"\nâŒ Error during test: {e}")
        print("\n" + "="*60)
        print("  âŒ TEST FAILED!")
        print("="*60)
        print("\n")
        return False

if __name__ == '__main__':
    success = test_gemini_connection()
    exit(0 if success else 1)
