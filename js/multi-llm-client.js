/**
 * Multi-LLM Client
 * Claude, OpenAI, Gemini ë©€í‹° í”„ë¡œë°”ì´ë” ì§€ì›
 * GitHub Pages ì •ì  í˜¸ìŠ¤íŒ… í˜¸í™˜
 */

// AppStorage - ì»¤ìŠ¤í…€ ìŠ¤í† ë¦¬ì§€ ìœ í‹¸ë¦¬í‹°
// (window.StorageëŠ” ë¸Œë¼ìš°ì € ë‚´ì¥ ì¸í„°í˜ì´ìŠ¤ë¼ ë®ì–´ì“¸ ìˆ˜ ì—†ìŒ)
const AppStorage = window.AppStorage || {
    get: (key) => {
        try {
            const item = localStorage.getItem(key);
            return item ? JSON.parse(item) : null;
        } catch (e) {
            return localStorage.getItem(key);
        }
    },
    set: (key, value) => {
        try {
            localStorage.setItem(key, typeof value === 'string' ? value : JSON.stringify(value));
        } catch (e) {
            console.error('AppStorage.set error:', e);
        }
    }
};
window.AppStorage = AppStorage;

// DateHelper fallback (config.jsì—ì„œ ì´ë¯¸ ì„ ì–¸ëœ ê²½ìš° ì¬ì„ ì–¸í•˜ì§€ ì•ŠìŒ)
if (!window.DateHelper) {
    window.DateHelper = {
        formatYYMMDD_hhmmss: () => {
            const now = new Date();
            return now.toISOString().replace(/[-:T]/g, '').substring(0, 14);
        },
        formatISO: () => new Date().toISOString()
    };
}

// ì „ì—­ ê°ì²´ ì°¸ì¡° (config.jsì—ì„œ ë¡œë“œëœ ê²½ìš° ì‚¬ìš©)

// LLM í”„ë¡œë°”ì´ë” ì •ì˜
const LLM_PROVIDERS = {
    claude: {
        name: 'Anthropic Claude',
        endpoint: 'https://api.anthropic.com/v1/messages',
        models: {
            'claude-opus-4-5': {
                name: 'Claude Opus 4.5',
                inputPrice: 15,
                outputPrice: 75,
                maxTokens: 16000,
                quality: 'highest',
                description: 'ìµœê³  í’ˆì§ˆ - í•µì‹¬ ë¶„ì„/í‰ê°€'
            },
            'claude-sonnet-3-5': {
                name: 'Claude Sonnet 3.5',
                inputPrice: 3,
                outputPrice: 15,
                maxTokens: 12000,
                quality: 'high',
                description: 'ê· í˜• - ì´ˆì•ˆ ì‘ì„±'
            },
            'claude-haiku-3-5': {
                name: 'Claude Haiku 3.5',
                inputPrice: 0.80,
                outputPrice: 4,
                maxTokens: 8000,
                quality: 'fast',
                description: 'ë¹ ë¥¸ ì²˜ë¦¬ - ë¶„ë¥˜/ê²€ì¦'
            }
        },
        defaultModel: 'claude-sonnet-3-5',
        apiKeyName: 'ANTHROPIC_API_KEY'
    },
    openai: {
        name: 'OpenAI',
        endpoint: 'https://api.openai.com/v1/chat/completions',
        models: {
            'gpt-4o': {
                name: 'GPT-4o',
                inputPrice: 5,
                outputPrice: 15,
                maxTokens: 12000,
                quality: 'high',
                description: 'ê· í˜• - ë²”ìš©'
            },
            'gpt-4o-mini': {
                name: 'GPT-4o Mini',
                inputPrice: 0.15,
                outputPrice: 0.60,
                maxTokens: 8000,
                quality: 'fast',
                description: 'ë¹ ë¥¸ ì²˜ë¦¬ - ê²½ì œì '
            }
        },
        defaultModel: 'gpt-4o',
        apiKeyName: 'OPENAI_API_KEY'
    },
    google: {
        name: 'Google Gemini',
        endpoint: 'https://generativelanguage.googleapis.com/v1beta/models',
        models: {
            'gemini-2.5-pro': {
                name: 'Gemini 2.5 Pro',
                inputPrice: 1.25,
                outputPrice: 5,
                maxTokens: 65536,
                quality: 'highest',
                description: 'ìµœê³  í’ˆì§ˆ - ì‹¬ì¸µ ë¶„ì„'
            },
            'gemini-2.0-flash-exp': {
                name: 'Gemini 2.5 Flash',
                inputPrice: 0.075,
                outputPrice: 0.30,
                maxTokens: 65536,
                quality: 'fast',
                description: 'ë¹ ë¥¸ ì²˜ë¦¬ - ë§¤ìš° ê²½ì œì '
            },
            'gemini-2.0-flash-exp': {
                name: 'Gemini 2.0 Flash Exp',
                inputPrice: 0,
                outputPrice: 0,
                maxTokens: 8192,
                quality: 'fast',
                description: 'ì‹¤í—˜ìš© - ë¬´ë£Œ'
            },
            'gemini-2.0-pro-exp': {
                name: 'Gemini 2.0 Pro Exp',
                inputPrice: 0,
                outputPrice: 0,
                maxTokens: 8192,
                quality: 'high',
                description: 'ì‹¤í—˜ìš© Pro - ë¬´ë£Œ'
            },
            'gemini-2.0-flash': {
                name: 'Gemini 2.0 Flash',
                inputPrice: 0,
                outputPrice: 0,
                maxTokens: 65536,
                quality: 'fast',
                description: 'Flash - ë¹ ë¥´ê³  íš¨ìœ¨ì '
            }
        },
        defaultModel: 'gemini-2.0-flash',
        apiKeyName: 'GOOGLE_API_KEY'
    }
};

// Hybrid ëª¨ë“œ ì„¤ì •
const HYBRID_MODES = {
    'sonnet-opus': {
        name: 'Sonnet â†’ Opus (ê¶Œì¥)',
        phase1: { provider: 'claude', model: 'claude-sonnet-3-5', description: 'ì „ì²´ ì´ˆì•ˆ' },
        phase2: { provider: 'claude', model: 'claude-opus-4-5', description: 'í•µì‹¬ ì„¹ì…˜ ê°œì„ ' },
        refineSections: [9, 10],
        estimatedSavings: 61
    },
    'haiku-sonnet': {
        name: 'Haiku â†’ Sonnet (ê²½ì œì )',
        phase1: { provider: 'claude', model: 'claude-haiku-3-5', description: 'ì „ì²´ ì´ˆì•ˆ' },
        phase2: { provider: 'claude', model: 'claude-sonnet-3-5', description: 'í•µì‹¬ ì„¹ì…˜ ê°œì„ ' },
        refineSections: [9, 10],
        estimatedSavings: 75
    },
    'gemini-opus': {
        name: 'Gemini â†’ Opus (ì´ˆê²½ì œì )',
        phase1: { provider: 'google', model: 'gemini-2.0-flash-exp', description: 'ì „ì²´ ì´ˆì•ˆ' },
        phase2: { provider: 'claude', model: 'claude-opus-4-5', description: 'í•µì‹¬ ì„¹ì…˜ ê°œì„ ' },
        refineSections: [9, 10],
        estimatedSavings: 80
    }
};

class MultiLLMClient {
    constructor() {
        this.dialogHistory = [];
        this.totalCost = 0;
        this.currentMode = 'single'; // 'single' or 'hybrid'
        this.hybridConfig = null;
    }

    // API í‚¤ ê´€ë¦¬
    setApiKey(provider, apiKey) {
        const keyName = LLM_PROVIDERS[provider]?.apiKeyName;
        if (keyName) {
            AppStorage.set(keyName, apiKey);
            console.log(`âœ… ${provider} API key set`);
            return true;
        }
        return false;
    }

    getApiKey(provider) {
        const keyName = LLM_PROVIDERS[provider]?.apiKeyName;
        return keyName ? AppStorage.get(keyName) : null;
    }

    hasApiKey(provider) {
        return !!this.getApiKey(provider);
    }

    // í”„ë¡œë°”ì´ë” ì •ë³´ ì¡°íšŒ
    getProviders() {
        return LLM_PROVIDERS;
    }

    getModels(provider) {
        return LLM_PROVIDERS[provider]?.models || {};
    }

    getHybridModes() {
        return HYBRID_MODES;
    }

    // ë¹„ìš© ê³„ì‚°
    estimateCost(provider, model, inputTokens, outputTokens) {
        const modelInfo = LLM_PROVIDERS[provider]?.models[model];
        if (!modelInfo) return 0;

        const inputCost = (inputTokens / 1000000) * modelInfo.inputPrice;
        const outputCost = (outputTokens / 1000000) * modelInfo.outputPrice;
        return inputCost + outputCost;
    }

    // Claude API í˜¸ì¶œ
    async callClaude(prompt, options = {}) {
        const apiKey = this.getApiKey('claude');
        if (!apiKey) throw new Error('Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');

        const model = options.model || LLM_PROVIDERS.claude.defaultModel;
        const modelInfo = LLM_PROVIDERS.claude.models[model];

        const response = await fetch(LLM_PROVIDERS.claude.endpoint, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'x-api-key': apiKey,
                'anthropic-version': '2023-06-01'
            },
            body: JSON.stringify({
                model: model,
                max_tokens: options.maxTokens || modelInfo.maxTokens,
                temperature: options.temperature || 0.3,
                messages: [{ role: 'user', content: prompt }]
            })
        });

        if (!response.ok) {
            const error = await response.json();
            throw new Error(`Claude API Error: ${error.error?.message || response.statusText}`);
        }

        const data = await response.json();
        const text = data.content?.[0]?.text || '';

        // ë¹„ìš© ê³„ì‚°
        const inputTokens = data.usage?.input_tokens || 0;
        const outputTokens = data.usage?.output_tokens || 0;
        const cost = this.estimateCost('claude', model, inputTokens, outputTokens);
        this.totalCost += cost;

        return {
            success: true,
            text,
            model,
            provider: 'claude',
            usage: { inputTokens, outputTokens },
            cost
        };
    }

    // OpenAI API í˜¸ì¶œ
    async callOpenAI(prompt, options = {}) {
        const apiKey = this.getApiKey('openai');
        if (!apiKey) throw new Error('OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');

        const model = options.model || LLM_PROVIDERS.openai.defaultModel;
        const modelInfo = LLM_PROVIDERS.openai.models[model];

        const response = await fetch(LLM_PROVIDERS.openai.endpoint, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
                model: model,
                max_tokens: options.maxTokens || modelInfo.maxTokens,
                temperature: options.temperature || 0.3,
                messages: [{ role: 'user', content: prompt }]
            })
        });

        if (!response.ok) {
            const error = await response.json();
            throw new Error(`OpenAI API Error: ${error.error?.message || response.statusText}`);
        }

        const data = await response.json();
        const text = data.choices?.[0]?.message?.content || '';

        const inputTokens = data.usage?.prompt_tokens || 0;
        const outputTokens = data.usage?.completion_tokens || 0;
        const cost = this.estimateCost('openai', model, inputTokens, outputTokens);
        this.totalCost += cost;

        return {
            success: true,
            text,
            model,
            provider: 'openai',
            usage: { inputTokens, outputTokens },
            cost
        };
    }

    // Gemini API í˜¸ì¶œ
    async callGemini(prompt, options = {}) {
        const apiKey = this.getApiKey('google');
        if (!apiKey) throw new Error('Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');

        const model = options.model || LLM_PROVIDERS.google.defaultModel;
        const url = `${LLM_PROVIDERS.google.endpoint}/${model}:generateContent?key=${apiKey}`;

        const response = await fetch(url, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                contents: [{ parts: [{ text: prompt }] }],
                generationConfig: {
                    temperature: options.temperature || 0.3,
                    maxOutputTokens: options.maxTokens || 8192
                }
            })
        });

        if (!response.ok) {
            const error = await response.json();
            throw new Error(`Gemini API Error: ${error.error?.message || response.statusText}`);
        }

        const data = await response.json();
        const text = data.candidates?.[0]?.content?.parts?.[0]?.text || '';

        // GeminiëŠ” usage ì •ë³´ê°€ ë‹¤ë¦„
        const inputTokens = data.usageMetadata?.promptTokenCount || 0;
        const outputTokens = data.usageMetadata?.candidatesTokenCount || 0;
        const cost = this.estimateCost('google', model, inputTokens, outputTokens);
        this.totalCost += cost;

        return {
            success: true,
            text,
            model,
            provider: 'google',
            usage: { inputTokens, outputTokens },
            cost
        };
    }

    // í†µí•© í˜¸ì¶œ ë©”ì„œë“œ
    async generate(prompt, options = {}) {
        const provider = options.provider || 'claude';
        const startTime = Date.now();

        let result;
        switch (provider) {
            case 'claude':
                result = await this.callClaude(prompt, options);
                break;
            case 'openai':
                result = await this.callOpenAI(prompt, options);
                break;
            case 'google':
                result = await this.callGemini(prompt, options);
                break;
            default:
                throw new Error(`Unknown provider: ${provider}`);
        }

        result.duration = ((Date.now() - startTime) / 1000).toFixed(2);

        // ëŒ€í™” ë¡œê·¸ ì €ì¥
        this.logDialog(prompt, result);

        return result;
    }

    // Hybrid ëª¨ë“œ ìƒì„±
    async generateHybrid(prompt, hybridMode, options = {}) {
        const config = HYBRID_MODES[hybridMode];
        if (!config) throw new Error(`Unknown hybrid mode: ${hybridMode}`);

        const results = {
            phase1: null,
            phase2: null,
            merged: null,
            totalCost: 0,
            totalDuration: 0
        };

        // Phase 1: ì „ì²´ ì´ˆì•ˆ ìƒì„±
        console.log(`ğŸš€ Phase 1: ${config.phase1.description} (${config.phase1.model})`);
        results.phase1 = await this.generate(prompt, {
            provider: config.phase1.provider,
            model: config.phase1.model,
            temperature: options.temperature || 0.5
        });
        results.totalCost += results.phase1.cost;
        results.totalDuration += parseFloat(results.phase1.duration);

        if (options.onPhase1Complete) {
            options.onPhase1Complete(results.phase1);
        }

        // Phase 2: í•µì‹¬ ì„¹ì…˜ ê°œì„ 
        const refinementPrompt = this.buildRefinementPrompt(
            results.phase1.text,
            config.refineSections,
            options.context
        );

        console.log(`âœ¨ Phase 2: ${config.phase2.description} (${config.phase2.model})`);
        results.phase2 = await this.generate(refinementPrompt, {
            provider: config.phase2.provider,
            model: config.phase2.model,
            temperature: options.temperature || 0.3
        });
        results.totalCost += results.phase2.cost;
        results.totalDuration += parseFloat(results.phase2.duration);

        if (options.onPhase2Complete) {
            options.onPhase2Complete(results.phase2);
        }

        // ê²°ê³¼ ë³‘í•©
        results.merged = this.mergeResults(
            results.phase1.text,
            results.phase2.text,
            config.refineSections
        );

        return results;
    }

    // Phase 2ìš© ê°œì„  í”„ë¡¬í”„íŠ¸ ìƒì„±
    buildRefinementPrompt(draft, sections, context) {
        const sectionNames = {
            9: 'ì¢…í•©ì ì¸ ì•ˆì „ì„± í‰ê°€',
            10: 'ê²°ë¡ '
        };

        const sectionList = sections.map(s => `${s}. ${sectionNames[s] || `ì„¹ì…˜ ${s}`}`).join(', ');

        return `ë‹¹ì‹ ì€ PSUR(ì •ê¸° ì•ˆì „ì„± ê°±ì‹  ë³´ê³ ì„œ) ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” ì´ˆì•ˆìœ¼ë¡œ ìƒì„±ëœ PSUR ë³´ê³ ì„œì…ë‹ˆë‹¤. ë‹¤ìŒ í•µì‹¬ ì„¹ì…˜ë“¤ì„ ì „ë¬¸ê°€ ìˆ˜ì¤€ìœ¼ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”:
**ê°œì„  ëŒ€ìƒ ì„¹ì…˜**: ${sectionList}

**ê°œì„  ì§€ì¹¨**:
1. ì •ëŸ‰ì  ë¶„ì„ ê°•í™” (Patient-years ê³„ì‚°, ë°œìƒë¥  ë¶„ì„)
2. Signal Detection ë°©ë²•ë¡  ëª…ì‹œ (PRR, ROR ë“±)
3. ê·œì œ ìš”ê±´ì— ë§ëŠ” êµ¬ì¡°í™”ëœ í‰ê°€
4. SOCë³„ ì²´ê³„ì  ë¶„ë¥˜
5. ìœ ìµì„±-ìœ„í•´ì„± ê· í˜• ì‹¬ì¸µ ë¶„ì„

${context ? `**ì°¸ê³  ì»¨í…ìŠ¤íŠ¸**:\n${context}\n` : ''}

**ì´ˆì•ˆ ë³´ê³ ì„œ**:
${draft}

**ì¶œë ¥ í˜•ì‹**:
ê°œì„ ëœ ì„¹ì…˜ë“¤ë§Œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”. ê° ì„¹ì…˜ì€ "## {ì„¹ì…˜ë²ˆí˜¸}. {ì„¹ì…˜ëª…}" í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.`;
    }

    // ê²°ê³¼ ë³‘í•©
    mergeResults(draft, refined, refineSections) {
        let result = draft;

        // ê°œì„ ëœ ê° ì„¹ì…˜ ì¶”ì¶œ ë° ë³‘í•©
        for (const sectionNum of refineSections) {
            const sectionPattern = new RegExp(
                `## ${sectionNum}\\.[^#]*?(?=## \\d+\\.|$)`,
                's'
            );

            const refinedMatch = refined.match(sectionPattern);
            if (refinedMatch) {
                result = result.replace(sectionPattern, refinedMatch[0]);
            }
        }

        return result;
    }

    // ëŒ€í™” ë¡œê·¸ ì €ì¥
    logDialog(prompt, result) {
        this.dialogHistory.push({
            timestamp: DateHelper.formatISO(),
            prompt: prompt.substring(0, 500) + (prompt.length > 500 ? '...' : ''),
            response: result.text?.substring(0, 500) + (result.text?.length > 500 ? '...' : ''),
            model: result.model,
            provider: result.provider,
            duration: result.duration,
            cost: result.cost,
            usage: result.usage
        });
    }

    // ëŒ€í™” ë¡œê·¸ ë‚´ë³´ë‚´ê¸°
    exportDialogHistory(reportName) {
        const timestamp = DateHelper.formatYYMMDD_hhmmss();
        const filename = `${reportName}_LLMLog_${timestamp}.md`;

        let markdown = `# LLM Dialog History: ${reportName}\n\n`;
        markdown += `**ìƒì„± ì‹œê°„**: ${DateHelper.formatISO()}\n`;
        markdown += `**ì´ ë¹„ìš©**: $${this.totalCost.toFixed(4)}\n\n`;
        markdown += `---\n\n`;

        this.dialogHistory.forEach((dialog, index) => {
            markdown += `## Dialog ${index + 1}\n\n`;
            markdown += `- **ì‹œê°„**: ${dialog.timestamp}\n`;
            markdown += `- **í”„ë¡œë°”ì´ë”**: ${dialog.provider}\n`;
            markdown += `- **ëª¨ë¸**: ${dialog.model}\n`;
            markdown += `- **ì†Œìš” ì‹œê°„**: ${dialog.duration}s\n`;
            markdown += `- **ë¹„ìš©**: $${dialog.cost?.toFixed(4) || 'N/A'}\n`;
            markdown += `- **í† í°**: ì…ë ¥ ${dialog.usage?.inputTokens || 0}, ì¶œë ¥ ${dialog.usage?.outputTokens || 0}\n\n`;
            markdown += `### Prompt (ì¼ë¶€)\n\`\`\`\n${dialog.prompt}\n\`\`\`\n\n`;
            markdown += `### Response (ì¼ë¶€)\n\`\`\`\n${dialog.response}\n\`\`\`\n\n`;
            markdown += `---\n\n`;
        });

        return { filename, content: markdown };
    }

    // í†µê³„ ì¡°íšŒ
    getStats() {
        return {
            totalCost: this.totalCost,
            dialogCount: this.dialogHistory.length,
            history: this.dialogHistory
        };
    }

    // ì´ˆê¸°í™”
    reset() {
        this.dialogHistory = [];
        this.totalCost = 0;
    }

    // í˜¸í™˜ì„± ë˜í¼: sendMessage (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ìš©)
    async sendMessage(prompt, options = {}) {
        // ê¸°ë³¸ providerë¥¼ googleë¡œ ì„¤ì • (Gemini ì‚¬ìš©)
        const provider = options.provider || 'google';
        const model = options.model || (provider === 'google' ? 'gemini-2.0-flash-exp' : undefined);

        try {
            const result = await this.generate(prompt, {
                ...options,
                provider,
                model
            });

            return {
                content: result.text,
                success: result.success,
                model: result.model,
                provider: result.provider,
                usage: result.usage,
                cost: result.cost
            };
        } catch (error) {
            console.error('LLM sendMessage error:', error);
            return {
                content: '',
                success: false,
                error: error.message
            };
        }
    }

    // í˜¸í™˜ì„± ë˜í¼: generateContent (llm-client.js í˜¸í™˜ìš©)
    async generateContent(prompt, options = {}) {
        return this.sendMessage(prompt, options);
    }
}

// Singleton instance
const multiLLMClient = new MultiLLMClient();

// ì „ì—­ ë‚´ë³´ë‚´ê¸° (ES6 ëª¨ë“ˆ ëŒ€ì‹ )
if (typeof window !== 'undefined') {
    window.multiLLMClient = multiLLMClient;
    window.MultiLLMClient = MultiLLMClient;
    window.LLM_PROVIDERS = LLM_PROVIDERS;
    window.HYBRID_MODES = HYBRID_MODES;
}
